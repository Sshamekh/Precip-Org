{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e5ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 13:53:42.150984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /glade/u/apps/ch/opt/mpt_fmods/2.25/intel/19.1.1:/glade/u/apps/ch/opt/mpt/2.25/lib:/glade/u/apps/opt/intel/2020u1/compilers_and_libraries/linux/lib/intel64:/glade/u/apps/ch/os/usr/lib64:/glade/u/apps/ch/os/usr/lib:/glade/u/apps/ch/os/lib64:/glade/u/apps/ch/os/lib\n",
      "2022-12-02 13:53:42.151023: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from utils_vae import read_field\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# import keras \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "# import sys, importlib\n",
    "\n",
    "# from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense,Flatten,Concatenate ,Conv1D\n",
    "from VAES import dense_gen,cloud_model,schedule\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f85331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading high resolution input\n",
      "qvi\n",
      "no scaling\n",
      "(760, 24, 287, 32, 32, 1)\n",
      "Reading large-scale inputs\n",
      "qvi\n",
      "ts\n",
      "tas\n",
      "huss\n",
      "(760, 24, 287, 4)\n",
      "Reading large-scale inputs2\n",
      "vas\n",
      "uas\n",
      "hfss\n",
      "hfls\n",
      "(760, 24, 287, 4)\n",
      "Reading large-scale outputs\n",
      "pracc\n",
      "(760, 24, 287, 1)\n",
      "[[[[0.8208742125920748]]]]\n"
     ]
    }
   ],
   "source": [
    "path = \"/glade/scratch/sshamekh/dyamond/SAM_highres/\"\n",
    "\n",
    "# %%time\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['utils_vae'])\n",
    "from utils_vae import train_test_data,read_data_for_z\n",
    "t1 = 0\n",
    "t2 = 8\n",
    "high_res = 4.0 # km \n",
    "large_scale = 128 #km\n",
    "x_hr,x_lg,x_lg2, y_out = read_data_for_z(path, high_res,large_scale,t1,t2,threshold_precip = 0.01,mask_threshold = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776e5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_anomaly = x_hr - np.mean(x_hr,axis = (3,4),keepdims=True)\n",
    "x_lg = np.concatenate((x_lg,x_lg2[:,:,:,2:]),axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 14:10:15.881583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /glade/u/apps/ch/opt/mpt_fmods/2.25/intel/19.1.1:/glade/u/apps/ch/opt/mpt/2.25/lib:/glade/u/apps/opt/intel/2020u1/compilers_and_libraries/linux/lib/intel64:/glade/u/apps/ch/os/usr/lib64:/glade/u/apps/ch/os/usr/lib:/glade/u/apps/ch/os/lib64:/glade/u/apps/ch/os/lib\n",
      "2022-12-02 14:10:15.881640: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-02 14:10:15.881678: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (r6i6n31): /proc/driver/nvidia/version does not exist\n",
      "2022-12-02 14:10:15.933218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "126138/163590 [======================>.......] - ETA: 31s"
     ]
    }
   ],
   "source": [
    "def make_z(hrdata,lgdata):\n",
    "    path = '/glade/u/home/sshamekh/results/dyamond/4nodes/'\n",
    "    encoder_result= keras.models.load_model(path + 'encoder_4_32_mse_anomaly')\n",
    "    precip_nn =  keras.models.load_model(path + 'precip_dense_vae_4_32_mse_anomaly')\n",
    "    \n",
    "    tt,lat,lon,nx,ny,ff = hrdata.shape\n",
    "    hrdata = hrdata.reshape(tt*lat*lon,nx,ny,ff)\n",
    "    lgd = lgdata.reshape(tt*lat*lon,-1)\n",
    "    \n",
    "    zall = encoder_result.predict(hrdata)\n",
    "    innew = np.concatenate((zall,lgd),axis = -1)\n",
    "    precip = precip_nn.predict(innew)\n",
    "    zall = zall.reshape(tt,lat,lon,zall.shape[-1])\n",
    "    return zall,precip\n",
    "\n",
    "z_true,precip = make_z(pw_anomaly*2,x_lg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c84dccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505, 3, 24, 287, 6) data_all\n",
      "(505, 3, 24, 287, 1) data_all\n",
      "(505, 3, 24, 287, 4) data_all\n",
      "(329, 24, 287, 3, 6)\n",
      "(176, 24, 287, 3, 6)\n",
      "(329, 24, 287, 3, 1)\n",
      "(176, 24, 287, 3, 1)\n",
      "(329, 24, 287, 3, 4)\n",
      "(176, 24, 287, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "def rand(nn):\n",
    "    import random\n",
    "    random.seed(9001)\n",
    "    x = [random.randint(1,100) for i in range(0,nn)]\n",
    "    return np.array(x)\n",
    "def chunk(data,window):\n",
    "    for i in range(0,window,2):\n",
    "        datain = np.copy(data[i:,:])\n",
    "        nt,nlat,nlon,nfeature = datain.shape\n",
    "        nchunk = int(nt/window)\n",
    "        dtin = datain[:nchunk*window ,:,:,:]\n",
    "        dtin = np.reshape(dtin,(nchunk,window,nlat,nlon,nfeature))\n",
    "        if i==0: \n",
    "            data_all = np.copy(dtin)\n",
    "        else: \n",
    "            data_all = np.concatenate((data_all,dtin),axis = 0 )\n",
    "    print (data_all.shape,'data_all')\n",
    "    return data_all\n",
    "\n",
    "\n",
    "\n",
    "def chunk_horizontal(data,window):\n",
    "    for i in range(window):\n",
    "        for j in range(window):\n",
    "            datain = np.copy(data[:,:,i:,j:])\n",
    "            nchunk,nw,nlat,nlon,nfeature = datain.shape\n",
    "            nch_lat = int(nlat/window)\n",
    "            nch_lon = int(nlon/window)\n",
    "            dtin = datain[:,:,:nch_lat*window,:nch_lon*window,:]\n",
    "            dtin = np.reshape(dtin,(nchunk,nw,nch_lat,window,nch_lon,window,nfeature))\n",
    "            print (dtin.shape)\n",
    "            dtin = np.moveaxis(dtin,3,4 )\n",
    "            if i==0 and j==0: \n",
    "                data_all = np.copy(dtin)\n",
    "            else: \n",
    "                data_all = np.concatenate((data_all,dtin),axis = 0 )\n",
    "        return data_all\n",
    "def reshape_data(data):\n",
    "    nt,window,nlat,nlon,nfeature = data.shape\n",
    "    \n",
    "    dtin = np.moveaxis(data,1,3)\n",
    "    print (dtin.shape)\n",
    "    dtin = np.reshape(dtin,(nlat*nlon*nt,window,nfeature))\n",
    "    return dtin\n",
    "\n",
    "timeframe=3\n",
    "\n",
    "\n",
    "x_all =  chunk(x_lg,timeframe)\n",
    "y_all =  chunk(y_out,timeframe)\n",
    "zz_all = chunk(z_true,timeframe)\n",
    "\n",
    "mask =  rand(y_all.shape[0]) < 70\n",
    "in_train,in_test = x_all[mask,:,:,:], x_all[~mask,:,:,:]\n",
    "out_train,out_test = y_all[mask,:,:,:], y_all[~mask,:,:,:]\n",
    "zz_train,zz_test = zz_all[mask,:,:,:], zz_all[~mask,:,:,:]\n",
    "\n",
    "x_test  = reshape_data (in_test)\n",
    "y_train = reshape_data(out_train)\n",
    "y_test = reshape_data(out_test)\n",
    "\n",
    "zz_train = reshape_data(zz_train)\n",
    "zz_test = reshape_data(zz_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741932fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (timeframe):\n",
    "    index = np.where(y_train[:,i,0]>.010)[0]\n",
    "    y_train = y_train[index,:]\n",
    "    x_train = x_train[index,:]\n",
    "    zz_train = zz_train[index,:]\n",
    "   \n",
    "\n",
    "    index = np.where(y_test[:,-1,0]>.010)[0]\n",
    "    y_test = y_test[index,:]\n",
    "    x_test = x_test[index,:]\n",
    "    zz_test = zz_test[index,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829d9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmzz = (np.mean(np.concatenate((zz_test[:,-1,:],zz_train[:,-1,:]),axis = 0),axis = 0,keepdims=True)).reshape(1,1,4)\n",
    "zzstd = (np.std(np.concatenate((zz_test[:,-1,:],zz_train[:,-1,:]),axis = 0),axis = 0,keepdims=True) ).reshape(1,1,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313e558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz_train = (zz_train - mmzz) / zzstd\n",
    "zz_test = (zz_test - mmzz) / zzstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "185ed109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.0469 - val_loss: 0.0450 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0366 - val_loss: 0.0482 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0365 - val_loss: 0.0455 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0366 - val_loss: 0.0449 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0365 - val_loss: 0.0457 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 6/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0365 - val_loss: 0.0447 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 7/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0364 - val_loss: 0.0455 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 8/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0364 - val_loss: 0.0452 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 9/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0364 - val_loss: 0.0451 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 10/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0363 - val_loss: 0.0506 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 11/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0364 - val_loss: 0.0449 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 12/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0363 - val_loss: 0.0449 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 13/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0363 - val_loss: 0.0449 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 14/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0363 - val_loss: 0.0451 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 15/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0362 - val_loss: 0.0449 - lr: 4.0000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0004.\n",
      "Epoch 16/100\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.0362 - val_loss: 0.0450 - lr: 4.0000e-04\n"
     ]
    }
   ],
   "source": [
    "units = [256,256,128,64]\n",
    "l1 = 0e-7\n",
    "\n",
    "initializer = None\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "activation = tf.nn.relu\n",
    "output_shape = 4\n",
    "inshapeX = x_train[0,2:,:].shape\n",
    "inzshape = zz_train[0,:2,:].shape\n",
    "\n",
    "val_split = 0.2\n",
    "\n",
    "\n",
    "input_lg = Input(shape=(inshapeX), name='inputs_lg')\n",
    "in_lg = Flatten()(input_lg)\n",
    "input_z = Input(shape=(inzshape), name='inputs_z')\n",
    "in_z = Flatten()(input_z)\n",
    "\n",
    "\n",
    "intotal = Concatenate(axis  = 1)([in_lg,in_z])\n",
    "\n",
    "xs =  Dense(units=units[0],activation = None,kernel_initializer=initializer,\n",
    "            name = 'dense1')(intotal)\n",
    "\n",
    "xs =  Dense(units=units[1],activation = activation,kernel_initializer=initializer,\n",
    "           name = 'dense2')(xs)\n",
    "\n",
    "xs =  Dense(units=units[2],activation = activation,kernel_initializer=initializer,\n",
    "           name = 'dense3')(xs)\n",
    "\n",
    "xs =  Dense(units=units[3],activation = activation,kernel_initializer=initializer,\n",
    "            name = 'dense4')(xs)\n",
    "\n",
    "y_predict =  Dense(units=output_shape,activation = None,name = 'prediction')(xs)\n",
    "\n",
    "nn_precip = Model(inputs = [input_lg,input_z],\n",
    "              outputs = [y_predict]) \n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "nn_precip.compile(optimizer = opt,loss = 'mse')\n",
    "\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "callback_lr=LearningRateScheduler(schedule,verbose=1)\n",
    "\n",
    "\n",
    "hist = nn_precip.fit({'inputs_lg' : x_train[:,2:,:],\n",
    "                      'inputs_z' : zz_train[:,:2,:],\n",
    "                   \n",
    "                      },\n",
    "                  {'prediction': zz_train[:,2,:]},\n",
    "                   epochs = epochs,  validation_split = val_split,callbacks= [earlyStopping,callback_lr],\n",
    "                     batch_size = batch_size,verbose = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "241a961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14997/14997 [==============================] - 11s 725us/step\n"
     ]
    }
   ],
   "source": [
    "zz_predict= nn_precip.predict([x_test[:,2:,:],zz_test[:,1:2,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74764a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2s = [r2_score(zz_test[:,2,0],zz_predict[:,0]),r2_score(zz_test[:,-1,1],zz_predict[:,1]),r2_score(zz_test[:,-1,2],zz_predict[:,2]),r2_score(zz_test[:,-1,3],zz_predict[:,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e851757a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9366492987966142,\n",
       "  0.9735186645212718,\n",
       "  0.9661567450141424,\n",
       "  0.9567288298220242],)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cc387038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_hist(x, y, ax, ax_histx, ax_histy,i,r2):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    \n",
    "    ax.tick_params(axis=\"both\", labelsize= 16)\n",
    "\n",
    "    ax_histy.set_xticks([])\n",
    "    # ax_histx.set_xticks([])\n",
    "    # ax_histy.set_yticks([])\n",
    "    ax_histx.set_yticks([])\n",
    "    ax_histx.spines['top'].set_visible(False)\n",
    "    ax_histx.spines['right'].set_visible(False)\n",
    "    ax_histx.spines['bottom'].set_visible(False)\n",
    "    ax_histx.spines['left'].set_visible(False)\n",
    "    ax_histy.spines['top'].set_visible(False)\n",
    "    ax_histy.spines['right'].set_visible(False)\n",
    "    ax_histy.spines['bottom'].set_visible(False)\n",
    "    ax_histy.spines['left'].set_visible(False)\n",
    "    ax.set_xlabel(r'$OrgT_{%s}$'%str(i),fontsize = 18)\n",
    "    ax.set_ylabel(r'$OrgP_{%s}$'%str(i),fontsize = 18)\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y)\n",
    "    ax.text(-1.2,1.2,r'$R^2 = %2.2f$'%r2, fontsize= 18)\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.01\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    # bins = 100\n",
    "    ax_histx.hist(x, bins=bins)\n",
    "    ax_histy.hist(y, bins=bins, orientation='horizontal')\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.set_xlim(-1.5,1.5)\n",
    "    \n",
    "def scatter_hist_2(x, y, ax, i,r2):\n",
    "    # no labels\n",
    "    ax_histx = ax.twinx()\n",
    "    ax_histy = ax.twiny()\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    \n",
    "    ax.tick_params(axis=\"both\", labelsize= 16)\n",
    "\n",
    "    ax_histy.set_xticks([])\n",
    "    # ax_histx.set_xticks([])\n",
    "    # ax_histy.set_yticks([])\n",
    "    ax_histx.set_yticks([])\n",
    "    ax_histx.spines['top'].set_visible(False)\n",
    "    ax_histx.spines['right'].set_visible(False)\n",
    "    ax_histx.spines['bottom'].set_visible(False)\n",
    "    ax_histx.spines['left'].set_visible(False)\n",
    "    ax_histy.spines['top'].set_visible(False)\n",
    "    ax_histy.spines['right'].set_visible(False)\n",
    "    ax_histy.spines['bottom'].set_visible(False)\n",
    "    ax_histy.spines['left'].set_visible(False)\n",
    "    ax.set_xlabel(r'$OrgT_{%s}$'%str(i),fontsize = 18)\n",
    "    ax.set_ylabel(r'$OrgP_{%s}$'%str(i),fontsize = 18)\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x[::6], y[::6],color = 'steelblue',alpha = 0.6,s = 60, facecolors='none')\n",
    "    ax.text(-.8,.8,r'$R^2 = %2.2f$'%r2, fontsize= 18)\n",
    "    ax.set_yticks(ticks=[-1,-0.5,0,0.5,1])\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.01\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    # bins = 100\n",
    "    ax_histx.hist(x, bins=bins,color = 'olivedrab',alpha = 0.8)\n",
    "    ax_histy.hist(y, bins=bins, orientation='horizontal',color = 'firebrick',alpha = 0.8)\n",
    "    ax.set_ylim(-1.,1.)\n",
    "    ax.set_xlim(-1.,1.)\n",
    "    ax_histy.set_xlim(0,120000)\n",
    "    ax_histx.set_ylim(0,120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099a847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
